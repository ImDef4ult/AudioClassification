{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import keras.utils.metrics_utils\n",
    "import numpy as np\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "max_pad_len = 174\n",
    "\n",
    "def extract_features(file_name):\n",
    "\n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast')\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "        pad_width = max_pad_len - mfccs.shape[1]\n",
    "        mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing file: \", file_name)\n",
    "        return None\n",
    "\n",
    "    return mfccs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3554it [02:16, 26.19it/s]C:\\Users\\masterdoc\\anaconda3\\envs\\DeepLearning\\lib\\site-packages\\librosa\\core\\spectrum.py:222: UserWarning: n_fft=2048 is too small for input signal of length=1323\n",
      "  warnings.warn(\n",
      "8323it [05:12, 34.51it/s]C:\\Users\\masterdoc\\anaconda3\\envs\\DeepLearning\\lib\\site-packages\\librosa\\core\\spectrum.py:222: UserWarning: n_fft=2048 is too small for input signal of length=1103\n",
      "  warnings.warn(\n",
      "C:\\Users\\masterdoc\\anaconda3\\envs\\DeepLearning\\lib\\site-packages\\librosa\\core\\spectrum.py:222: UserWarning: n_fft=2048 is too small for input signal of length=1523\n",
      "  warnings.warn(\n",
      "8732it [05:27, 26.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished feature extraction from  8732  files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the path to the full UrbanSound dataset\n",
    "full_dataset_path = 'Data/UrbanSound8K/audio'\n",
    "metadata = pd.read_csv('Data/UrbanSound8K/metadata/UrbanSound8K.csv')\n",
    "features = []\n",
    "\n",
    "# Iterate through each sound file and extract the features\n",
    "for index, row in tqdm(metadata.iterrows()):\n",
    "\n",
    "    file_name = os.path.join(os.path.abspath(full_dataset_path),'fold'+str(row[\"fold\"])+'/',str(row[\"slice_file_name\"]))\n",
    "\n",
    "    class_label = row[\"class\"]\n",
    "    data = extract_features(file_name)\n",
    "\n",
    "    features.append([data, class_label])\n",
    "\n",
    "# Convert into a Panda dataframe\n",
    "features_df = pd.DataFrame(features, columns=['feature','class_label'])\n",
    "\n",
    "print('Finished feature extraction from ', len(features_df), ' files')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                feature       class_label\n0     [[-306.77255, -177.59209, -99.13616, -65.97198...          dog_bark\n1     [[-457.6953, -451.0248, -450.68613, -444.99997...  children_playing\n2     [[-468.0367, -467.42264, -481.04654, -486.5948...  children_playing\n3     [[-422.42215, -411.9085, -409.46243, -409.0892...  children_playing\n4     [[-438.10162, -434.47787, -443.3284, -442.6644...  children_playing\n...                                                 ...               ...\n8727  [[-397.82446, -400.45578, -407.5035, -408.9529...          car_horn\n8728  [[-451.81265, -451.41983, -450.67892, -445.635...          car_horn\n8729  [[-301.06348, -298.25397, -305.0326, -303.8614...          car_horn\n8730  [[-373.6307, -369.44986, -366.48, -364.9094, -...          car_horn\n8731  [[-309.34647, -305.3132, -308.23593, -308.1856...          car_horn\n\n[8732 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature</th>\n      <th>class_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[[-306.77255, -177.59209, -99.13616, -65.97198...</td>\n      <td>dog_bark</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[[-457.6953, -451.0248, -450.68613, -444.99997...</td>\n      <td>children_playing</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[[-468.0367, -467.42264, -481.04654, -486.5948...</td>\n      <td>children_playing</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[[-422.42215, -411.9085, -409.46243, -409.0892...</td>\n      <td>children_playing</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[[-438.10162, -434.47787, -443.3284, -442.6644...</td>\n      <td>children_playing</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>8727</th>\n      <td>[[-397.82446, -400.45578, -407.5035, -408.9529...</td>\n      <td>car_horn</td>\n    </tr>\n    <tr>\n      <th>8728</th>\n      <td>[[-451.81265, -451.41983, -450.67892, -445.635...</td>\n      <td>car_horn</td>\n    </tr>\n    <tr>\n      <th>8729</th>\n      <td>[[-301.06348, -298.25397, -305.0326, -303.8614...</td>\n      <td>car_horn</td>\n    </tr>\n    <tr>\n      <th>8730</th>\n      <td>[[-373.6307, -369.44986, -366.48, -364.9094, -...</td>\n      <td>car_horn</td>\n    </tr>\n    <tr>\n      <th>8731</th>\n      <td>[[-309.34647, -305.3132, -308.23593, -308.1856...</td>\n      <td>car_horn</td>\n    </tr>\n  </tbody>\n</table>\n<p>8732 rows Ã— 2 columns</p>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "features_df.to_csv('features_df_index.csv', index=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Convert features and corresponding classification labels into numpy arrays\n",
    "X = np.array(features_df.feature.tolist())\n",
    "y = np.array(features_df.class_label.tolist())\n",
    "\n",
    "# Encode the classification labels\n",
    "le = LabelEncoder()\n",
    "yy = to_categorical(le.fit_transform(y))\n",
    "\n",
    "# split the dataset\n",
    "\n",
    "x_train_full, x_test, y_train_full, y_test = train_test_split(X, yy, test_size=0.2, random_state = 42)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train_full, y_train_full, test_size=0.2, random_state = 42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CNN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, Conv2D, GlobalAveragePooling2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn import metrics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "num_rows = 40\n",
    "num_columns = 174\n",
    "num_channels = 1\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], num_rows, num_columns, num_channels)\n",
    "x_val = x_val.reshape(x_val.shape[0], num_rows, num_columns, num_channels)\n",
    "x_test = x_test.reshape(x_test.shape[0], num_rows, num_columns, num_channels)\n",
    "\n",
    "num_labels = yy.shape[1]\n",
    "filter_size = 2\n",
    "\n",
    "# Construct model\n",
    "model_basic = Sequential()\n",
    "model_basic.add(Conv2D(filters=16, kernel_size=2, input_shape=(num_rows, num_columns, num_channels), activation='relu'))\n",
    "model_basic.add(MaxPooling2D(pool_size=2))\n",
    "model_basic.add(Dropout(0.2))\n",
    "\n",
    "model_basic.add(Conv2D(filters=32, kernel_size=2, activation='relu'))\n",
    "model_basic.add(MaxPooling2D(pool_size=2))\n",
    "model_basic.add(Dropout(0.2))\n",
    "\n",
    "model_basic.add(Conv2D(filters=64, kernel_size=2, activation='relu'))\n",
    "model_basic.add(MaxPooling2D(pool_size=2))\n",
    "model_basic.add(Dropout(0.2))\n",
    "\n",
    "model_basic.add(Conv2D(filters=128, kernel_size=2, activation='relu'))\n",
    "model_basic.add(MaxPooling2D(pool_size=2))\n",
    "model_basic.add(Dropout(0.2))\n",
    "model_basic.add(GlobalAveragePooling2D())\n",
    "\n",
    "model_basic.add(Dense(num_labels, activation='softmax'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Compile the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model_basic.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 39, 173, 16)       80        \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 19, 86, 16)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 19, 86, 16)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 18, 85, 32)        2080      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 9, 42, 32)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 9, 42, 32)         0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 8, 41, 64)         8256      \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 4, 20, 64)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 4, 20, 64)         0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 3, 19, 128)        32896     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 1, 9, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 1, 9, 128)         0         \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 128)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44,602\n",
      "Trainable params: 44,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "44/44 [==============================] - 3s 5ms/step - loss: 8.4576 - accuracy: 0.1274\n",
      "Pre-training accuracy: 12.7416%\n"
     ]
    }
   ],
   "source": [
    "# Display model architecture summary\n",
    "model_basic.summary()\n",
    "\n",
    "# Calculate pre-training accuracy\n",
    "score = model_basic.evaluate(x_val, y_val, verbose=1)\n",
    "accuracy = 100*score[1]\n",
    "\n",
    "print(\"Pre-training accuracy: %.4f%%\" % accuracy)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from datetime import datetime"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "num_epochs = 72\n",
    "num_batch_size = 256\n",
    "\n",
    "checkpointer = ModelCheckpoint(\n",
    "    filepath='saved_models_CNN/weights.best.basic_cnn.hdf5',\n",
    "    verbose=1,\n",
    "    save_best_only=True\n",
    ")\n",
    "start = datetime.now()\n",
    "model_basic.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(x_val, y_val), callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(f'Trained the model in: {duration}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 5.6332 - accuracy: 0.1705\n",
      "Epoch 00001: val_loss improved from inf to 2.52932, saving model to saved_models_CNN\\weights.best.basic_cnn.hdf5\n",
      "22/22 [==============================] - 2s 48ms/step - loss: 5.6332 - accuracy: 0.1705 - val_loss: 2.5293 - val_accuracy: 0.1954\n",
      "Epoch 2/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 2.2591 - accuracy: 0.2568\n",
      "Epoch 00002: val_loss improved from 2.52932 to 1.95529, saving model to saved_models_CNN\\weights.best.basic_cnn.hdf5\n",
      "22/22 [==============================] - 1s 25ms/step - loss: 2.2591 - accuracy: 0.2568 - val_loss: 1.9553 - val_accuracy: 0.3164\n",
      "Epoch 3/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.8939 - accuracy: 0.3312\n",
      "Epoch 00003: val_loss improved from 1.95529 to 1.85334, saving model to saved_models_CNN\\weights.best.basic_cnn.hdf5\n",
      "22/22 [==============================] - 1s 25ms/step - loss: 1.8939 - accuracy: 0.3312 - val_loss: 1.8533 - val_accuracy: 0.3457\n",
      "Epoch 4/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.7058 - accuracy: 0.4028\n",
      "Epoch 00004: val_loss improved from 1.85334 to 1.73950, saving model to saved_models_CNN\\weights.best.basic_cnn.hdf5\n",
      "22/22 [==============================] - 1s 25ms/step - loss: 1.7058 - accuracy: 0.4028 - val_loss: 1.7395 - val_accuracy: 0.3694\n",
      "Epoch 5/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.5709 - accuracy: 0.4511\n",
      "Epoch 00005: val_loss improved from 1.73950 to 1.64577, saving model to saved_models_CNN\\weights.best.basic_cnn.hdf5\n",
      "22/22 [==============================] - 1s 25ms/step - loss: 1.5709 - accuracy: 0.4511 - val_loss: 1.6458 - val_accuracy: 0.4173\n",
      "Epoch 6/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.4591 - accuracy: 0.4900\n",
      "Epoch 00006: val_loss improved from 1.64577 to 1.54686, saving model to saved_models_CNN\\weights.best.basic_cnn.hdf5\n",
      "22/22 [==============================] - 1s 25ms/step - loss: 1.4591 - accuracy: 0.4900 - val_loss: 1.5469 - val_accuracy: 0.4732\n",
      "Epoch 7/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.3641 - accuracy: 0.5254\n",
      "Epoch 00007: val_loss improved from 1.54686 to 1.42156, saving model to saved_models_CNN\\weights.best.basic_cnn.hdf5\n",
      "22/22 [==============================] - 1s 25ms/step - loss: 1.3641 - accuracy: 0.5254 - val_loss: 1.4216 - val_accuracy: 0.5247\n",
      "Epoch 8/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.3031 - accuracy: 0.5451\n",
      "Epoch 00008: val_loss improved from 1.42156 to 1.39937, saving model to saved_models_CNN\\weights.best.basic_cnn.hdf5\n",
      "22/22 [==============================] - 1s 26ms/step - loss: 1.3031 - accuracy: 0.5451 - val_loss: 1.3994 - val_accuracy: 0.5211\n",
      "Epoch 9/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.2383 - accuracy: 0.5666\n",
      "Epoch 00009: val_loss improved from 1.39937 to 1.33211, saving model to saved_models_CNN\\weights.best.basic_cnn.hdf5\n",
      "22/22 [==============================] - 1s 25ms/step - loss: 1.2383 - accuracy: 0.5666 - val_loss: 1.3321 - val_accuracy: 0.5412\n",
      "Epoch 10/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.1810 - accuracy: 0.5965\n",
      "Epoch 00010: val_loss improved from 1.33211 to 1.31043, saving model to saved_models_CNN\\weights.best.basic_cnn.hdf5\n",
      "22/22 [==============================] - 1s 25ms/step - loss: 1.1810 - accuracy: 0.5965 - val_loss: 1.3104 - val_accuracy: 0.5533\n",
      "Epoch 11/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.1489 - accuracy: 0.5979\n",
      "Epoch 00011: val_loss improved from 1.31043 to 1.26831, saving model to saved_models_CNN\\weights.best.basic_cnn.hdf5\n",
      "22/22 [==============================] - 1s 25ms/step - loss: 1.1489 - accuracy: 0.5979 - val_loss: 1.2683 - val_accuracy: 0.5698\n",
      "Epoch 12/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.0932 - accuracy: 0.6203\n",
      "Epoch 00012: val_loss improved from 1.26831 to 1.23575, saving model to saved_models_CNN\\weights.best.basic_cnn.hdf5\n",
      "22/22 [==============================] - 1s 25ms/step - loss: 1.0932 - accuracy: 0.6203 - val_loss: 1.2358 - val_accuracy: 0.5583\n",
      "Epoch 13/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.0597 - accuracy: 0.6324\n",
      "Epoch 00013: val_loss improved from 1.23575 to 1.17924, saving model to saved_models_CNN\\weights.best.basic_cnn.hdf5\n",
      "22/22 [==============================] - 1s 24ms/step - loss: 1.0597 - accuracy: 0.6324 - val_loss: 1.1792 - val_accuracy: 0.6013\n",
      "Epoch 14/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.0298 - accuracy: 0.6467\n",
      "Epoch 00014: val_loss improved from 1.17924 to 1.17289, saving model to saved_models_CNN\\weights.best.basic_cnn.hdf5\n",
      "22/22 [==============================] - 1s 25ms/step - loss: 1.0298 - accuracy: 0.6467 - val_loss: 1.1729 - val_accuracy: 0.6056\n",
      "Epoch 15/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.9886 - accuracy: 0.6575\n",
      "Epoch 00015: val_loss improved from 1.17289 to 1.13102, saving model to saved_models_CNN\\weights.best.basic_cnn.hdf5\n",
      "22/22 [==============================] - 1s 25ms/step - loss: 0.9886 - accuracy: 0.6575 - val_loss: 1.1310 - val_accuracy: 0.6314\n",
      "Epoch 16/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.9558 - accuracy: 0.6732\n",
      "Epoch 00016: val_loss improved from 1.13102 to 1.09144, saving model to saved_models_CNN\\weights.best.basic_cnn.hdf5\n",
      "22/22 [==============================] - 1s 25ms/step - loss: 0.9558 - accuracy: 0.6732 - val_loss: 1.0914 - val_accuracy: 0.6421\n",
      "Epoch 17/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.9302 - accuracy: 0.6793\n",
      "Epoch 00017: val_loss improved from 1.09144 to 1.03305, saving model to saved_models_CNN\\weights.best.basic_cnn.hdf5\n",
      "22/22 [==============================] - 1s 25ms/step - loss: 0.9302 - accuracy: 0.6793 - val_loss: 1.0331 - val_accuracy: 0.6779\n",
      "Epoch 18/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.8942 - accuracy: 0.6897\n",
      "Epoch 00018: val_loss improved from 1.03305 to 1.02527, saving model to saved_models_CNN\\weights.best.basic_cnn.hdf5\n",
      "22/22 [==============================] - 1s 25ms/step - loss: 0.8942 - accuracy: 0.6897 - val_loss: 1.0253 - val_accuracy: 0.6628\n",
      "Epoch 19/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.8699 - accuracy: 0.6999\n",
      "Epoch 00019: val_loss improved from 1.02527 to 0.98538, saving model to saved_models_CNN\\weights.best.basic_cnn.hdf5\n",
      "22/22 [==============================] - 1s 25ms/step - loss: 0.8699 - accuracy: 0.6999 - val_loss: 0.9854 - val_accuracy: 0.6872\n",
      "Epoch 20/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.8537 - accuracy: 0.7044\n",
      "Epoch 00020: val_loss improved from 0.98538 to 0.98443, saving model to saved_models_CNN\\weights.best.basic_cnn.hdf5\n",
      "22/22 [==============================] - 1s 25ms/step - loss: 0.8537 - accuracy: 0.7044 - val_loss: 0.9844 - val_accuracy: 0.6994\n",
      "Epoch 21/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.8165 - accuracy: 0.7267\n",
      "Epoch 00021: val_loss improved from 0.98443 to 0.97440, saving model to saved_models_CNN\\weights.best.basic_cnn.hdf5\n",
      "22/22 [==============================] - 1s 24ms/step - loss: 0.8165 - accuracy: 0.7267 - val_loss: 0.9744 - val_accuracy: 0.6872\n",
      "Epoch 22/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.8219 - accuracy: 0.7169\n",
      "Epoch 00022: val_loss improved from 0.97440 to 0.93003, saving model to saved_models_CNN\\weights.best.basic_cnn.hdf5\n",
      "22/22 [==============================] - 1s 25ms/step - loss: 0.8219 - accuracy: 0.7169 - val_loss: 0.9300 - val_accuracy: 0.7122\n",
      "Epoch 23/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.7837 - accuracy: 0.7335\n",
      "Epoch 00023: val_loss did not improve from 0.93003\n",
      "22/22 [==============================] - 1s 24ms/step - loss: 0.7837 - accuracy: 0.7335 - val_loss: 0.9510 - val_accuracy: 0.6915\n",
      "Epoch 24/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.7646 - accuracy: 0.7391\n",
      "Epoch 00024: val_loss improved from 0.93003 to 0.86230, saving model to saved_models_CNN\\weights.best.basic_cnn.hdf5\n",
      "22/22 [==============================] - 1s 25ms/step - loss: 0.7646 - accuracy: 0.7391 - val_loss: 0.8623 - val_accuracy: 0.7445\n",
      "Epoch 25/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.7405 - accuracy: 0.7443\n",
      "Epoch 00025: val_loss did not improve from 0.86230\n",
      "22/22 [==============================] - 1s 24ms/step - loss: 0.7405 - accuracy: 0.7443 - val_loss: 0.9134 - val_accuracy: 0.7244\n",
      "Epoch 26/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.7374 - accuracy: 0.7495\n",
      "Epoch 00026: val_loss improved from 0.86230 to 0.84312, saving model to saved_models_CNN\\weights.best.basic_cnn.hdf5\n",
      "22/22 [==============================] - 1s 25ms/step - loss: 0.7374 - accuracy: 0.7495 - val_loss: 0.8431 - val_accuracy: 0.7502\n",
      "Epoch 27/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.7249 - accuracy: 0.7473\n",
      "Epoch 00027: val_loss improved from 0.84312 to 0.81347, saving model to saved_models_CNN\\weights.best.basic_cnn.hdf5\n",
      "22/22 [==============================] - 1s 25ms/step - loss: 0.7249 - accuracy: 0.7473 - val_loss: 0.8135 - val_accuracy: 0.7659\n",
      "Epoch 28/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6987 - accuracy: 0.7597\n",
      "Epoch 00028: val_loss improved from 0.81347 to 0.79786, saving model to saved_models_CNN\\weights.best.basic_cnn.hdf5\n",
      "22/22 [==============================] - 1s 25ms/step - loss: 0.6987 - accuracy: 0.7597 - val_loss: 0.7979 - val_accuracy: 0.7573\n",
      "Epoch 29/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6944 - accuracy: 0.7602\n",
      "Epoch 00029: val_loss improved from 0.79786 to 0.79157, saving model to saved_models_CNN\\weights.best.basic_cnn.hdf5\n",
      "22/22 [==============================] - 1s 25ms/step - loss: 0.6944 - accuracy: 0.7602 - val_loss: 0.7916 - val_accuracy: 0.7581\n",
      "Epoch 30/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6608 - accuracy: 0.7754\n",
      "Epoch 00030: val_loss improved from 0.79157 to 0.78305, saving model to saved_models_CNN\\weights.best.basic_cnn.hdf5\n",
      "22/22 [==============================] - 1s 25ms/step - loss: 0.6608 - accuracy: 0.7754 - val_loss: 0.7831 - val_accuracy: 0.7559\n",
      "Epoch 31/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6491 - accuracy: 0.7758\n",
      "Epoch 00031: val_loss improved from 0.78305 to 0.75478, saving model to saved_models_CNN\\weights.best.basic_cnn.hdf5\n",
      "22/22 [==============================] - 1s 25ms/step - loss: 0.6491 - accuracy: 0.7758 - val_loss: 0.7548 - val_accuracy: 0.7681\n",
      "Epoch 32/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6224 - accuracy: 0.7845\n",
      "Epoch 00032: val_loss did not improve from 0.75478\n",
      "22/22 [==============================] - 1s 23ms/step - loss: 0.6224 - accuracy: 0.7845 - val_loss: 0.7874 - val_accuracy: 0.7581\n",
      "Epoch 33/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6296 - accuracy: 0.7867\n",
      "Epoch 00033: val_loss improved from 0.75478 to 0.72126, saving model to saved_models_CNN\\weights.best.basic_cnn.hdf5\n",
      "22/22 [==============================] - 1s 25ms/step - loss: 0.6296 - accuracy: 0.7867 - val_loss: 0.7213 - val_accuracy: 0.7802\n",
      "Epoch 34/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5960 - accuracy: 0.7938\n",
      "Epoch 00034: val_loss improved from 0.72126 to 0.69881, saving model to saved_models_CNN\\weights.best.basic_cnn.hdf5\n",
      "22/22 [==============================] - 1s 25ms/step - loss: 0.5960 - accuracy: 0.7938 - val_loss: 0.6988 - val_accuracy: 0.7874\n",
      "Epoch 35/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5948 - accuracy: 0.7972\n",
      "Epoch 00035: val_loss did not improve from 0.69881\n",
      "22/22 [==============================] - 1s 24ms/step - loss: 0.5948 - accuracy: 0.7972 - val_loss: 0.7363 - val_accuracy: 0.7709\n",
      "Epoch 36/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5900 - accuracy: 0.7951\n",
      "Epoch 00036: val_loss did not improve from 0.69881\n",
      "22/22 [==============================] - 1s 24ms/step - loss: 0.5900 - accuracy: 0.7951 - val_loss: 0.7001 - val_accuracy: 0.7838\n",
      "Epoch 37/72\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5813 - accuracy: 0.8032\n",
      "Epoch 00037: val_loss improved from 0.69881 to 0.69130, saving model to saved_models_CNN\\weights.best.basic_cnn.hdf5\n",
      "22/22 [==============================] - 1s 27ms/step - loss: 0.5815 - accuracy: 0.8030 - val_loss: 0.6913 - val_accuracy: 0.7867\n",
      "Epoch 38/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5542 - accuracy: 0.8125\n",
      "Epoch 00038: val_loss improved from 0.69130 to 0.67631, saving model to saved_models_CNN\\weights.best.basic_cnn.hdf5\n",
      "22/22 [==============================] - 1s 25ms/step - loss: 0.5542 - accuracy: 0.8125 - val_loss: 0.6763 - val_accuracy: 0.7817\n",
      "Epoch 39/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5501 - accuracy: 0.8125\n",
      "Epoch 00039: val_loss improved from 0.67631 to 0.64353, saving model to saved_models_CNN\\weights.best.basic_cnn.hdf5\n",
      "22/22 [==============================] - 1s 25ms/step - loss: 0.5501 - accuracy: 0.8125 - val_loss: 0.6435 - val_accuracy: 0.7931\n",
      "Epoch 40/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5439 - accuracy: 0.8171\n",
      "Epoch 00040: val_loss improved from 0.64353 to 0.63253, saving model to saved_models_CNN\\weights.best.basic_cnn.hdf5\n",
      "22/22 [==============================] - 1s 25ms/step - loss: 0.5439 - accuracy: 0.8171 - val_loss: 0.6325 - val_accuracy: 0.8017\n",
      "Epoch 41/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5341 - accuracy: 0.8160\n",
      "Epoch 00041: val_loss did not improve from 0.63253\n",
      "22/22 [==============================] - 1s 23ms/step - loss: 0.5341 - accuracy: 0.8160 - val_loss: 0.6672 - val_accuracy: 0.7853\n",
      "Epoch 42/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5321 - accuracy: 0.8146\n",
      "Epoch 00042: val_loss did not improve from 0.63253\n",
      "22/22 [==============================] - 1s 23ms/step - loss: 0.5321 - accuracy: 0.8146 - val_loss: 0.6358 - val_accuracy: 0.8046\n",
      "Epoch 43/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5120 - accuracy: 0.8212\n",
      "Epoch 00043: val_loss improved from 0.63253 to 0.63187, saving model to saved_models_CNN\\weights.best.basic_cnn.hdf5\n",
      "22/22 [==============================] - 1s 25ms/step - loss: 0.5120 - accuracy: 0.8212 - val_loss: 0.6319 - val_accuracy: 0.8010\n",
      "Epoch 44/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5128 - accuracy: 0.8218\n",
      "Epoch 00044: val_loss improved from 0.63187 to 0.62540, saving model to saved_models_CNN\\weights.best.basic_cnn.hdf5\n",
      "22/22 [==============================] - 1s 25ms/step - loss: 0.5128 - accuracy: 0.8218 - val_loss: 0.6254 - val_accuracy: 0.8074\n",
      "Epoch 45/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5047 - accuracy: 0.8239\n",
      "Epoch 00045: val_loss improved from 0.62540 to 0.58736, saving model to saved_models_CNN\\weights.best.basic_cnn.hdf5\n",
      "22/22 [==============================] - 1s 26ms/step - loss: 0.5047 - accuracy: 0.8239 - val_loss: 0.5874 - val_accuracy: 0.8182\n",
      "Epoch 46/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5138 - accuracy: 0.8180\n",
      "Epoch 00046: val_loss did not improve from 0.58736\n",
      "22/22 [==============================] - 1s 23ms/step - loss: 0.5138 - accuracy: 0.8180 - val_loss: 0.6436 - val_accuracy: 0.7981\n",
      "Epoch 47/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.4923 - accuracy: 0.8336\n",
      "Epoch 00047: val_loss improved from 0.58736 to 0.57838, saving model to saved_models_CNN\\weights.best.basic_cnn.hdf5\n",
      "22/22 [==============================] - 1s 25ms/step - loss: 0.4923 - accuracy: 0.8336 - val_loss: 0.5784 - val_accuracy: 0.8175\n",
      "Epoch 48/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.4744 - accuracy: 0.8343\n",
      "Epoch 00048: val_loss did not improve from 0.57838\n",
      "22/22 [==============================] - 1s 23ms/step - loss: 0.4744 - accuracy: 0.8343 - val_loss: 0.6017 - val_accuracy: 0.8125\n",
      "Epoch 49/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.4708 - accuracy: 0.8354\n",
      "Epoch 00049: val_loss did not improve from 0.57838\n",
      "22/22 [==============================] - 1s 23ms/step - loss: 0.4708 - accuracy: 0.8354 - val_loss: 0.5875 - val_accuracy: 0.8096\n",
      "Epoch 50/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.4594 - accuracy: 0.8431\n",
      "Epoch 00050: val_loss did not improve from 0.57838\n",
      "22/22 [==============================] - 1s 23ms/step - loss: 0.4594 - accuracy: 0.8431 - val_loss: 0.6122 - val_accuracy: 0.7974\n",
      "Epoch 51/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.4568 - accuracy: 0.8380\n",
      "Epoch 00051: val_loss improved from 0.57838 to 0.57102, saving model to saved_models_CNN\\weights.best.basic_cnn.hdf5\n",
      "22/22 [==============================] - 1s 25ms/step - loss: 0.4568 - accuracy: 0.8380 - val_loss: 0.5710 - val_accuracy: 0.8168\n",
      "Epoch 52/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.4462 - accuracy: 0.8400\n",
      "Epoch 00052: val_loss improved from 0.57102 to 0.55058, saving model to saved_models_CNN\\weights.best.basic_cnn.hdf5\n",
      "22/22 [==============================] - 1s 25ms/step - loss: 0.4462 - accuracy: 0.8400 - val_loss: 0.5506 - val_accuracy: 0.8246\n",
      "Epoch 53/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.4436 - accuracy: 0.8416\n",
      "Epoch 00053: val_loss did not improve from 0.55058\n",
      "22/22 [==============================] - 1s 23ms/step - loss: 0.4436 - accuracy: 0.8416 - val_loss: 0.5803 - val_accuracy: 0.8089\n",
      "Epoch 54/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.4294 - accuracy: 0.8484\n",
      "Epoch 00054: val_loss did not improve from 0.55058\n",
      "22/22 [==============================] - 1s 23ms/step - loss: 0.4294 - accuracy: 0.8484 - val_loss: 0.5961 - val_accuracy: 0.8039\n",
      "Epoch 55/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.4198 - accuracy: 0.8576\n",
      "Epoch 00055: val_loss improved from 0.55058 to 0.54260, saving model to saved_models_CNN\\weights.best.basic_cnn.hdf5\n",
      "22/22 [==============================] - 1s 25ms/step - loss: 0.4198 - accuracy: 0.8576 - val_loss: 0.5426 - val_accuracy: 0.8168\n",
      "Epoch 56/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.4072 - accuracy: 0.8554\n",
      "Epoch 00056: val_loss improved from 0.54260 to 0.53255, saving model to saved_models_CNN\\weights.best.basic_cnn.hdf5\n",
      "22/22 [==============================] - 1s 25ms/step - loss: 0.4072 - accuracy: 0.8554 - val_loss: 0.5326 - val_accuracy: 0.8239\n",
      "Epoch 57/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.4175 - accuracy: 0.8552\n",
      "Epoch 00057: val_loss did not improve from 0.53255\n",
      "22/22 [==============================] - 1s 23ms/step - loss: 0.4175 - accuracy: 0.8552 - val_loss: 0.5464 - val_accuracy: 0.8196\n",
      "Epoch 58/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.4117 - accuracy: 0.8568\n",
      "Epoch 00058: val_loss did not improve from 0.53255\n",
      "22/22 [==============================] - 1s 23ms/step - loss: 0.4117 - accuracy: 0.8568 - val_loss: 0.5427 - val_accuracy: 0.8311\n",
      "Epoch 59/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.4030 - accuracy: 0.8577\n",
      "Epoch 00059: val_loss improved from 0.53255 to 0.51704, saving model to saved_models_CNN\\weights.best.basic_cnn.hdf5\n",
      "22/22 [==============================] - 1s 25ms/step - loss: 0.4030 - accuracy: 0.8577 - val_loss: 0.5170 - val_accuracy: 0.8346\n",
      "Epoch 60/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.3823 - accuracy: 0.8656\n",
      "Epoch 00060: val_loss did not improve from 0.51704\n",
      "22/22 [==============================] - 1s 23ms/step - loss: 0.3823 - accuracy: 0.8656 - val_loss: 0.5384 - val_accuracy: 0.8275\n",
      "Epoch 61/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.3795 - accuracy: 0.8672\n",
      "Epoch 00061: val_loss did not improve from 0.51704\n",
      "22/22 [==============================] - 1s 23ms/step - loss: 0.3795 - accuracy: 0.8672 - val_loss: 0.5348 - val_accuracy: 0.8246\n",
      "Epoch 62/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.3900 - accuracy: 0.8640\n",
      "Epoch 00062: val_loss improved from 0.51704 to 0.50780, saving model to saved_models_CNN\\weights.best.basic_cnn.hdf5\n",
      "22/22 [==============================] - 1s 25ms/step - loss: 0.3900 - accuracy: 0.8640 - val_loss: 0.5078 - val_accuracy: 0.8389\n",
      "Epoch 63/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.3800 - accuracy: 0.8669\n",
      "Epoch 00063: val_loss improved from 0.50780 to 0.49411, saving model to saved_models_CNN\\weights.best.basic_cnn.hdf5\n",
      "22/22 [==============================] - 1s 25ms/step - loss: 0.3800 - accuracy: 0.8669 - val_loss: 0.4941 - val_accuracy: 0.8332\n",
      "Epoch 64/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.3578 - accuracy: 0.8686\n",
      "Epoch 00064: val_loss did not improve from 0.49411\n",
      "22/22 [==============================] - 1s 23ms/step - loss: 0.3578 - accuracy: 0.8686 - val_loss: 0.5157 - val_accuracy: 0.8346\n",
      "Epoch 65/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.3692 - accuracy: 0.8690\n",
      "Epoch 00065: val_loss improved from 0.49411 to 0.49092, saving model to saved_models_CNN\\weights.best.basic_cnn.hdf5\n",
      "22/22 [==============================] - 1s 25ms/step - loss: 0.3692 - accuracy: 0.8690 - val_loss: 0.4909 - val_accuracy: 0.8404\n",
      "Epoch 66/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.3548 - accuracy: 0.8754\n",
      "Epoch 00066: val_loss did not improve from 0.49092\n",
      "22/22 [==============================] - 1s 23ms/step - loss: 0.3548 - accuracy: 0.8754 - val_loss: 0.5177 - val_accuracy: 0.8289\n",
      "Epoch 67/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.3523 - accuracy: 0.8765\n",
      "Epoch 00067: val_loss did not improve from 0.49092\n",
      "22/22 [==============================] - 1s 23ms/step - loss: 0.3523 - accuracy: 0.8765 - val_loss: 0.5225 - val_accuracy: 0.8318\n",
      "Epoch 68/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.3336 - accuracy: 0.8817\n",
      "Epoch 00068: val_loss did not improve from 0.49092\n",
      "22/22 [==============================] - 1s 23ms/step - loss: 0.3336 - accuracy: 0.8817 - val_loss: 0.5306 - val_accuracy: 0.8268\n",
      "Epoch 69/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.3483 - accuracy: 0.8790\n",
      "Epoch 00069: val_loss improved from 0.49092 to 0.48921, saving model to saved_models_CNN\\weights.best.basic_cnn.hdf5\n",
      "22/22 [==============================] - 1s 25ms/step - loss: 0.3483 - accuracy: 0.8790 - val_loss: 0.4892 - val_accuracy: 0.8418\n",
      "Epoch 70/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.3313 - accuracy: 0.8803\n",
      "Epoch 00070: val_loss improved from 0.48921 to 0.47975, saving model to saved_models_CNN\\weights.best.basic_cnn.hdf5\n",
      "22/22 [==============================] - 1s 25ms/step - loss: 0.3313 - accuracy: 0.8803 - val_loss: 0.4798 - val_accuracy: 0.8504\n",
      "Epoch 71/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.3452 - accuracy: 0.8760\n",
      "Epoch 00071: val_loss did not improve from 0.47975\n",
      "22/22 [==============================] - 1s 23ms/step - loss: 0.3452 - accuracy: 0.8760 - val_loss: 0.5088 - val_accuracy: 0.8225\n",
      "Epoch 72/72\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.3332 - accuracy: 0.8814\n",
      "Epoch 00072: val_loss did not improve from 0.47975\n",
      "22/22 [==============================] - 1s 23ms/step - loss: 0.3332 - accuracy: 0.8814 - val_loss: 0.5348 - val_accuracy: 0.8296\n",
      "Trained the model in: 0:00:40.190156\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Metrics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.9005010724067688\n",
      "Testing Accuracy:  0.8296349048614502\n"
     ]
    }
   ],
   "source": [
    "score = model_basic.evaluate(x_train, y_train, verbose=0)\n",
    "print(\"Training Accuracy: \", score[1])\n",
    "\n",
    "score = model_basic.evaluate(x_val, y_val, verbose=0)\n",
    "print(\"Testing Accuracy: \", score[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Confusion matrix"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, classification_report"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def confusion_matrix(x_test, y_test, target):\n",
    "    predicted_vector = target.predict(x_test)\n",
    "    classes_x = np.argmax(predicted_vector, axis=1)\n",
    "    predicted_class = le.inverse_transform(classes_x)\n",
    "\n",
    "    classes_y = np.argmax(y_test, axis=1)\n",
    "    true_class = le.inverse_transform(classes_y)\n",
    "    print(classification_report(true_class, predicted_class))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      " air_conditioner       0.84      0.96      0.90       203\n",
      "        car_horn       0.74      0.98      0.84        86\n",
      "children_playing       0.83      0.66      0.74       183\n",
      "        dog_bark       0.99      0.63      0.77       201\n",
      "        drilling       0.86      0.86      0.86       206\n",
      "   engine_idling       0.84      0.93      0.88       193\n",
      "        gun_shot       0.90      0.96      0.93        72\n",
      "      jackhammer       0.92      0.95      0.94       208\n",
      "           siren       0.80      0.94      0.86       165\n",
      "    street_music       0.78      0.77      0.77       230\n",
      "\n",
      "        accuracy                           0.85      1747\n",
      "       macro avg       0.85      0.86      0.85      1747\n",
      "    weighted avg       0.85      0.85      0.84      1747\n",
      "\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix(x_test, y_test, model_basic)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Heatmap"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix as cf\n",
    "def heat_map(x_test, y_test, target):\n",
    "    predicted_vector = target.predict(x_test)\n",
    "    classes_x = np.argmax(predicted_vector, axis=1)\n",
    "    predicted_class = le.inverse_transform(classes_x)\n",
    "\n",
    "    classes_y = np.argmax(y_test, axis=1)\n",
    "    true_class = le.inverse_transform(classes_y)\n",
    "    ax = sns.heatmap(cf(true_class, predicted_class))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "heat_map(x_test, y_test, model_basic)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Predictions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "def print_prediction(file_name, target):\n",
    "    prediction_feature = extract_features(file_name)\n",
    "    prediction_feature = prediction_feature.reshape(1, num_rows, num_columns, num_channels)\n",
    "\n",
    "    predicted_vector = target.predict(prediction_feature)\n",
    "    classes_x = np.argmax(predicted_vector, axis=1)\n",
    "    predicted_class = le.inverse_transform(classes_x)\n",
    "    print(\"The predicted class is:\", predicted_class[0], '\\n')\n",
    "\n",
    "    predicted_proba_vector = target.predict(prediction_feature)\n",
    "    predicted_proba = predicted_proba_vector[0]\n",
    "    for i in range(len(predicted_proba)):\n",
    "        category = le.inverse_transform(np.array([i]))\n",
    "        print(category[0], \"\\t\\t : \", format(predicted_proba[i], '.32f'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class is: air_conditioner \n",
      "\n",
      "air_conditioner \t\t :  0.93232434988021850585937500000000\n",
      "car_horn \t\t :  0.00002944457264675293117761611938\n",
      "children_playing \t\t :  0.00198938813991844654083251953125\n",
      "dog_bark \t\t :  0.00001823407365009188652038574219\n",
      "drilling \t\t :  0.00667552230879664421081542968750\n",
      "engine_idling \t\t :  0.02055497467517852783203125000000\n",
      "gun_shot \t\t :  0.00015482208982575684785842895508\n",
      "jackhammer \t\t :  0.03789340332150459289550781250000\n",
      "siren \t\t :  0.00030287238769233226776123046875\n",
      "street_music \t\t :  0.00005689940735464915633201599121\n"
     ]
    }
   ],
   "source": [
    "# Air conditioner\n",
    "file_name = 'samples/100852-0-0-0.wav'\n",
    "print_prediction(file_name, model_basic)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class is: drilling \n",
      "\n",
      "air_conditioner \t\t :  0.00000243364092966658063232898712\n",
      "car_horn \t\t :  0.00000258209252024244051426649094\n",
      "children_playing \t\t :  0.00001591095860931091010570526123\n",
      "dog_bark \t\t :  0.00001640665323066059499979019165\n",
      "drilling \t\t :  0.99586063623428344726562500000000\n",
      "engine_idling \t\t :  0.00007234972144942730665206909180\n",
      "gun_shot \t\t :  0.00000568887935514794662594795227\n",
      "jackhammer \t\t :  0.00052923709154129028320312500000\n",
      "siren \t\t :  0.00000022276857691849727416411042\n",
      "street_music \t\t :  0.00349460612051188945770263671875\n"
     ]
    }
   ],
   "source": [
    "# Drilling\n",
    "file_name = 'samples/103199-4-0-0.wav'\n",
    "print_prediction(file_name, model_basic)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class is: street_music \n",
      "\n",
      "air_conditioner \t\t :  0.00113205052912235260009765625000\n",
      "car_horn \t\t :  0.00691472319886088371276855468750\n",
      "children_playing \t\t :  0.02257700636982917785644531250000\n",
      "dog_bark \t\t :  0.00057672831462696194648742675781\n",
      "drilling \t\t :  0.00000466042138214106671512126923\n",
      "engine_idling \t\t :  0.00001532563692308031022548675537\n",
      "gun_shot \t\t :  0.00000000006087955833899982849289\n",
      "jackhammer \t\t :  0.00000135088612296385690569877625\n",
      "siren \t\t :  0.00485296547412872314453125000000\n",
      "street_music \t\t :  0.96392518281936645507812500000000\n"
     ]
    }
   ],
   "source": [
    "# Street music\n",
    "file_name = 'samples/101848-9-0-0.wav'\n",
    "print_prediction(file_name, model_basic)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class is: drilling \n",
      "\n",
      "air_conditioner \t\t :  0.00188682472798973321914672851562\n",
      "car_horn \t\t :  0.24110135436058044433593750000000\n",
      "children_playing \t\t :  0.00853456463664770126342773437500\n",
      "dog_bark \t\t :  0.14143005013465881347656250000000\n",
      "drilling \t\t :  0.24255423247814178466796875000000\n",
      "engine_idling \t\t :  0.00945129245519638061523437500000\n",
      "gun_shot \t\t :  0.11836099624633789062500000000000\n",
      "jackhammer \t\t :  0.21975129842758178710937500000000\n",
      "siren \t\t :  0.01350787747651338577270507812500\n",
      "street_music \t\t :  0.00342152873054146766662597656250\n"
     ]
    }
   ],
   "source": [
    "# Car horn\n",
    "file_name = 'samples/100648-1-0-0.wav'\n",
    "print_prediction(file_name, model_basic)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Save model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "# Serialize model to Json\n",
    "model_json = model_basic.to_json()\n",
    "with open('models/cnn.json', 'w') as json_file:\n",
    "    json_file.write(model_json)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "# Serialize weights to HDF5\n",
    "model_basic.save_weights('models/cnn.h5')\n",
    "print('Model saved')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load the model and test it"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "json_file2 = open('models/cnn.json')\n",
    "loaded_model_json = json_file2.read()\n",
    "json_file2.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# Load weights into new model\n",
    "loaded_model.load_weights('models/cnn.h5')\n",
    "print('Model loaded')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test the loaded model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class is: street_music \n",
      "\n",
      "air_conditioner \t\t :  0.00113205052912235260009765625000\n",
      "car_horn \t\t :  0.00691472319886088371276855468750\n",
      "children_playing \t\t :  0.02257700636982917785644531250000\n",
      "dog_bark \t\t :  0.00057672831462696194648742675781\n",
      "drilling \t\t :  0.00000466042138214106671512126923\n",
      "engine_idling \t\t :  0.00001532563692308031022548675537\n",
      "gun_shot \t\t :  0.00000000006087955833899982849289\n",
      "jackhammer \t\t :  0.00000135088612296385690569877625\n",
      "siren \t\t :  0.00485296547412872314453125000000\n",
      "street_music \t\t :  0.96392518281936645507812500000000\n"
     ]
    }
   ],
   "source": [
    "file_name = 'samples/101848-9-0-0.wav'\n",
    "prediction_feature = extract_features(file_name)\n",
    "prediction_feature = prediction_feature.reshape(1, num_rows, num_columns, num_channels)\n",
    "\n",
    "predicted_vector = loaded_model.predict(prediction_feature)\n",
    "classes_x = np.argmax(predicted_vector, axis=1)\n",
    "predicted_class = le.inverse_transform(classes_x)\n",
    "print(\"The predicted class is:\", predicted_class[0], '\\n')\n",
    "\n",
    "predicted_proba_vector = loaded_model.predict(prediction_feature)\n",
    "predicted_proba = predicted_proba_vector[0]\n",
    "for i in range(len(predicted_proba)):\n",
    "    category = le.inverse_transform(np.array([i]))\n",
    "    print(category[0], \"\\t\\t : \", format(predicted_proba[i], '.32f'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tunning the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "def cnn_tunning():\n",
    "    results = pd.DataFrame(columns=['epochs', 'kernel_size', 'train', 'val', 'time'])\n",
    "    kernel_list = [8, 16, 32]\n",
    "    epochs_list = [50, 100, 150]\n",
    "    for kernel in kernel_list:\n",
    "        for epoch in epochs_list:\n",
    "            print(f'Training model: Kernel -> {kernel} - Epochs -> {epoch}')\n",
    "            # Construct model\n",
    "            model = Sequential()\n",
    "            model.add(Conv2D(filters=kernel, kernel_size=2, input_shape=(num_rows, num_columns, num_channels), activation='relu'))\n",
    "            model.add(MaxPooling2D(pool_size=2))\n",
    "            model.add(Dropout(0.2))\n",
    "\n",
    "            model.add(Conv2D(filters=kernel*2, kernel_size=2, activation='relu'))\n",
    "            model.add(MaxPooling2D(pool_size=2))\n",
    "            model.add(Dropout(0.2))\n",
    "\n",
    "            model.add(Conv2D(filters=kernel*3, kernel_size=2, activation='relu'))\n",
    "            model.add(MaxPooling2D(pool_size=2))\n",
    "            model.add(Dropout(0.2))\n",
    "\n",
    "            model.add(Conv2D(filters=kernel*4, kernel_size=2, activation='relu'))\n",
    "            model.add(MaxPooling2D(pool_size=2))\n",
    "            model.add(Dropout(0.2))\n",
    "            model.add(GlobalAveragePooling2D())\n",
    "\n",
    "            model.add(Dense(num_labels, activation='softmax'))\n",
    "\n",
    "            # Compile the model\n",
    "            model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "            checkpointer = ModelCheckpoint(\n",
    "                filepath='saved_models_CNN/weights.best.basic_cnn.hdf5',\n",
    "                verbose=1,\n",
    "                save_best_only=True\n",
    "            )\n",
    "            start = datetime.now()\n",
    "            model.fit(x_train, y_train, batch_size=num_batch_size, epochs=epoch, validation_data=(x_val, y_val), verbose=0)\n",
    "\n",
    "            duration = datetime.now() - start\n",
    "\n",
    "            score_train = model.evaluate(x_train, y_train, verbose=0)\n",
    "\n",
    "            score_val = model.evaluate(x_val, y_val, verbose=0)\n",
    "            results.loc[len(results)] = [epoch, kernel, score_train[1], score_val[1], duration]\n",
    "    return results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model: Kernel -> 8 - Epochs -> 50\n",
      "Training model: Kernel -> 8 - Epochs -> 100\n",
      "Training model: Kernel -> 8 - Epochs -> 150\n",
      "Training model: Kernel -> 16 - Epochs -> 50\n",
      "Training model: Kernel -> 16 - Epochs -> 100\n",
      "Training model: Kernel -> 16 - Epochs -> 150\n",
      "Training model: Kernel -> 32 - Epochs -> 50\n",
      "Training model: Kernel -> 32 - Epochs -> 100\n",
      "Training model: Kernel -> 32 - Epochs -> 150\n"
     ]
    }
   ],
   "source": [
    "tunning_results = cnn_tunning()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "  epochs kernel_size     train       val                   time\n0     50           8  0.608805  0.581246 0 days 00:00:19.657822\n1    100           8  0.726020  0.690050 0 days 00:00:36.898316\n2    150           8  0.762527  0.730852 0 days 00:00:53.667048\n3     50          16  0.826235  0.797423 0 days 00:00:24.213938\n4    100          16  0.901933  0.843235 0 days 00:00:47.145021\n5    150          16  0.947208  0.871152 0 days 00:01:08.602828\n6     50          32  0.927881  0.875447 0 days 00:00:41.869337\n7    100          32  0.987115  0.921976 0 days 00:01:19.543911\n8    150          32  0.993737  0.921260 0 days 00:02:01.024631",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>epochs</th>\n      <th>kernel_size</th>\n      <th>train</th>\n      <th>val</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>50</td>\n      <td>8</td>\n      <td>0.608805</td>\n      <td>0.581246</td>\n      <td>0 days 00:00:19.657822</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>100</td>\n      <td>8</td>\n      <td>0.726020</td>\n      <td>0.690050</td>\n      <td>0 days 00:00:36.898316</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>150</td>\n      <td>8</td>\n      <td>0.762527</td>\n      <td>0.730852</td>\n      <td>0 days 00:00:53.667048</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>50</td>\n      <td>16</td>\n      <td>0.826235</td>\n      <td>0.797423</td>\n      <td>0 days 00:00:24.213938</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>100</td>\n      <td>16</td>\n      <td>0.901933</td>\n      <td>0.843235</td>\n      <td>0 days 00:00:47.145021</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>150</td>\n      <td>16</td>\n      <td>0.947208</td>\n      <td>0.871152</td>\n      <td>0 days 00:01:08.602828</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>50</td>\n      <td>32</td>\n      <td>0.927881</td>\n      <td>0.875447</td>\n      <td>0 days 00:00:41.869337</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>100</td>\n      <td>32</td>\n      <td>0.987115</td>\n      <td>0.921976</td>\n      <td>0 days 00:01:19.543911</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>150</td>\n      <td>32</td>\n      <td>0.993737</td>\n      <td>0.921260</td>\n      <td>0 days 00:02:01.024631</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tunning_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing the tunned model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "# Construct model\n",
    "model_tunned = Sequential()\n",
    "model_tunned.add(Conv2D(filters=32, kernel_size=2, input_shape=(num_rows, num_columns, num_channels), activation='relu'))\n",
    "model_tunned.add(MaxPooling2D(pool_size=2))\n",
    "model_tunned.add(Dropout(0.2))\n",
    "\n",
    "model_tunned.add(Conv2D(filters=64, kernel_size=2, activation='relu'))\n",
    "model_tunned.add(MaxPooling2D(pool_size=2))\n",
    "model_tunned.add(Dropout(0.2))\n",
    "\n",
    "model_tunned.add(Conv2D(filters=96, kernel_size=2, activation='relu'))\n",
    "model_tunned.add(MaxPooling2D(pool_size=2))\n",
    "model_tunned.add(Dropout(0.2))\n",
    "\n",
    "model_tunned.add(Conv2D(filters=128, kernel_size=2, activation='relu'))\n",
    "model_tunned.add(MaxPooling2D(pool_size=2))\n",
    "model_tunned.add(Dropout(0.2))\n",
    "model_tunned.add(GlobalAveragePooling2D())\n",
    "\n",
    "model_tunned.add(Dense(num_labels, activation='softmax'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 4.6280 - accuracy: 0.1650 - val_loss: 2.1679 - val_accuracy: 0.2169\n",
      "Epoch 2/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 1.9161 - accuracy: 0.3178 - val_loss: 1.9022 - val_accuracy: 0.3930\n",
      "Epoch 3/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 1.6070 - accuracy: 0.4284 - val_loss: 1.6388 - val_accuracy: 0.4603\n",
      "Epoch 4/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 1.4272 - accuracy: 0.4918 - val_loss: 1.5301 - val_accuracy: 0.4882\n",
      "Epoch 5/150\n",
      "22/22 [==============================] - 1s 39ms/step - loss: 1.3247 - accuracy: 0.5408 - val_loss: 1.4607 - val_accuracy: 0.5025\n",
      "Epoch 6/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 1.2301 - accuracy: 0.5702 - val_loss: 1.3190 - val_accuracy: 0.5540\n",
      "Epoch 7/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 1.1532 - accuracy: 0.5966 - val_loss: 1.2195 - val_accuracy: 0.6034\n",
      "Epoch 8/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 1.0736 - accuracy: 0.6328 - val_loss: 1.1541 - val_accuracy: 0.6342\n",
      "Epoch 9/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 1.0281 - accuracy: 0.6450 - val_loss: 1.1220 - val_accuracy: 0.6342\n",
      "Epoch 10/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.9878 - accuracy: 0.6596 - val_loss: 1.0370 - val_accuracy: 0.6893\n",
      "Epoch 11/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.9486 - accuracy: 0.6707 - val_loss: 1.0726 - val_accuracy: 0.6571\n",
      "Epoch 12/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.9251 - accuracy: 0.6836 - val_loss: 1.0115 - val_accuracy: 0.6757\n",
      "Epoch 13/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.8507 - accuracy: 0.7049 - val_loss: 0.9247 - val_accuracy: 0.7008\n",
      "Epoch 14/150\n",
      "22/22 [==============================] - 1s 39ms/step - loss: 0.8029 - accuracy: 0.7267 - val_loss: 0.8827 - val_accuracy: 0.7273\n",
      "Epoch 15/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.7912 - accuracy: 0.7323 - val_loss: 0.8574 - val_accuracy: 0.7215\n",
      "Epoch 16/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.7270 - accuracy: 0.7586 - val_loss: 0.8418 - val_accuracy: 0.7344\n",
      "Epoch 17/150\n",
      "22/22 [==============================] - 1s 39ms/step - loss: 0.7400 - accuracy: 0.7443 - val_loss: 0.8386 - val_accuracy: 0.7316\n",
      "Epoch 18/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.7136 - accuracy: 0.7534 - val_loss: 0.7833 - val_accuracy: 0.7473\n",
      "Epoch 19/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.6762 - accuracy: 0.7684 - val_loss: 0.7489 - val_accuracy: 0.7437\n",
      "Epoch 20/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.6457 - accuracy: 0.7822 - val_loss: 0.7167 - val_accuracy: 0.7731\n",
      "Epoch 21/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.6380 - accuracy: 0.7835 - val_loss: 0.6932 - val_accuracy: 0.7860\n",
      "Epoch 22/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.5917 - accuracy: 0.8001 - val_loss: 0.6912 - val_accuracy: 0.7831\n",
      "Epoch 23/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.5937 - accuracy: 0.8006 - val_loss: 0.6583 - val_accuracy: 0.7974\n",
      "Epoch 24/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.5815 - accuracy: 0.8015 - val_loss: 0.6424 - val_accuracy: 0.8074\n",
      "Epoch 25/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.5601 - accuracy: 0.8116 - val_loss: 0.6465 - val_accuracy: 0.7974\n",
      "Epoch 26/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.5195 - accuracy: 0.8210 - val_loss: 0.6478 - val_accuracy: 0.7938\n",
      "Epoch 27/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.5190 - accuracy: 0.8253 - val_loss: 0.5818 - val_accuracy: 0.8268\n",
      "Epoch 28/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.5136 - accuracy: 0.8286 - val_loss: 0.6111 - val_accuracy: 0.7946\n",
      "Epoch 29/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.4793 - accuracy: 0.8361 - val_loss: 0.5672 - val_accuracy: 0.8175\n",
      "Epoch 30/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.4662 - accuracy: 0.8447 - val_loss: 0.5576 - val_accuracy: 0.8346\n",
      "Epoch 31/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.4593 - accuracy: 0.8447 - val_loss: 0.5506 - val_accuracy: 0.8311\n",
      "Epoch 32/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.4476 - accuracy: 0.8477 - val_loss: 0.5317 - val_accuracy: 0.8346\n",
      "Epoch 33/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.4181 - accuracy: 0.8579 - val_loss: 0.5188 - val_accuracy: 0.8404\n",
      "Epoch 34/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.4125 - accuracy: 0.8631 - val_loss: 0.4868 - val_accuracy: 0.8533\n",
      "Epoch 35/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.4277 - accuracy: 0.8540 - val_loss: 0.4804 - val_accuracy: 0.8533\n",
      "Epoch 36/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.3941 - accuracy: 0.8665 - val_loss: 0.4559 - val_accuracy: 0.8611\n",
      "Epoch 37/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.3684 - accuracy: 0.8801 - val_loss: 0.4832 - val_accuracy: 0.8504\n",
      "Epoch 38/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.3752 - accuracy: 0.8756 - val_loss: 0.4420 - val_accuracy: 0.8618\n",
      "Epoch 39/150\n",
      "22/22 [==============================] - 1s 39ms/step - loss: 0.3629 - accuracy: 0.8756 - val_loss: 0.4387 - val_accuracy: 0.8719\n",
      "Epoch 40/150\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.3506 - accuracy: 0.8776 - val_loss: 0.4421 - val_accuracy: 0.8647\n",
      "Epoch 41/150\n",
      "22/22 [==============================] - 1s 39ms/step - loss: 0.3476 - accuracy: 0.8819 - val_loss: 0.4371 - val_accuracy: 0.8669\n",
      "Epoch 42/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.3292 - accuracy: 0.8901 - val_loss: 0.4597 - val_accuracy: 0.8719\n",
      "Epoch 43/150\n",
      "22/22 [==============================] - 1s 39ms/step - loss: 0.3099 - accuracy: 0.8941 - val_loss: 0.4438 - val_accuracy: 0.8669\n",
      "Epoch 44/150\n",
      "22/22 [==============================] - 1s 39ms/step - loss: 0.3166 - accuracy: 0.8930 - val_loss: 0.4119 - val_accuracy: 0.8898\n",
      "Epoch 45/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.3195 - accuracy: 0.8933 - val_loss: 0.4287 - val_accuracy: 0.8747\n",
      "Epoch 46/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.3042 - accuracy: 0.8962 - val_loss: 0.4123 - val_accuracy: 0.8876\n",
      "Epoch 47/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.2822 - accuracy: 0.9064 - val_loss: 0.4147 - val_accuracy: 0.8747\n",
      "Epoch 48/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.2787 - accuracy: 0.9064 - val_loss: 0.3794 - val_accuracy: 0.8941\n",
      "Epoch 49/150\n",
      "22/22 [==============================] - 1s 39ms/step - loss: 0.2806 - accuracy: 0.9023 - val_loss: 0.4315 - val_accuracy: 0.8640\n",
      "Epoch 50/150\n",
      "22/22 [==============================] - 1s 39ms/step - loss: 0.2805 - accuracy: 0.9030 - val_loss: 0.4017 - val_accuracy: 0.8805\n",
      "Epoch 51/150\n",
      "22/22 [==============================] - 1s 39ms/step - loss: 0.2634 - accuracy: 0.9111 - val_loss: 0.3754 - val_accuracy: 0.8948\n",
      "Epoch 52/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.2706 - accuracy: 0.9057 - val_loss: 0.3835 - val_accuracy: 0.8869\n",
      "Epoch 53/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.2471 - accuracy: 0.9157 - val_loss: 0.3890 - val_accuracy: 0.8941\n",
      "Epoch 54/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.2553 - accuracy: 0.9162 - val_loss: 0.3721 - val_accuracy: 0.8790\n",
      "Epoch 55/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.2487 - accuracy: 0.9127 - val_loss: 0.3623 - val_accuracy: 0.9041\n",
      "Epoch 56/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.2519 - accuracy: 0.9123 - val_loss: 0.3748 - val_accuracy: 0.8933\n",
      "Epoch 57/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.2533 - accuracy: 0.9112 - val_loss: 0.3970 - val_accuracy: 0.8826\n",
      "Epoch 58/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.2305 - accuracy: 0.9166 - val_loss: 0.3494 - val_accuracy: 0.9098\n",
      "Epoch 59/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.2174 - accuracy: 0.9225 - val_loss: 0.3340 - val_accuracy: 0.9141\n",
      "Epoch 60/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.2120 - accuracy: 0.9291 - val_loss: 0.3596 - val_accuracy: 0.8941\n",
      "Epoch 61/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.2185 - accuracy: 0.9248 - val_loss: 0.3679 - val_accuracy: 0.8933\n",
      "Epoch 62/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.2168 - accuracy: 0.9264 - val_loss: 0.3807 - val_accuracy: 0.8905\n",
      "Epoch 63/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.2126 - accuracy: 0.9304 - val_loss: 0.3513 - val_accuracy: 0.9041\n",
      "Epoch 64/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.1976 - accuracy: 0.9297 - val_loss: 0.3464 - val_accuracy: 0.9120\n",
      "Epoch 65/150\n",
      "22/22 [==============================] - 1s 39ms/step - loss: 0.1768 - accuracy: 0.9386 - val_loss: 0.3225 - val_accuracy: 0.9148\n",
      "Epoch 66/150\n",
      "22/22 [==============================] - 1s 39ms/step - loss: 0.1951 - accuracy: 0.9340 - val_loss: 0.3634 - val_accuracy: 0.8969\n",
      "Epoch 67/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.1889 - accuracy: 0.9350 - val_loss: 0.3206 - val_accuracy: 0.9134\n",
      "Epoch 68/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.2000 - accuracy: 0.9298 - val_loss: 0.3298 - val_accuracy: 0.9141\n",
      "Epoch 69/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.1767 - accuracy: 0.9359 - val_loss: 0.3547 - val_accuracy: 0.9048\n",
      "Epoch 70/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.1825 - accuracy: 0.9377 - val_loss: 0.3693 - val_accuracy: 0.9005\n",
      "Epoch 71/150\n",
      "22/22 [==============================] - 1s 39ms/step - loss: 0.1860 - accuracy: 0.9361 - val_loss: 0.3507 - val_accuracy: 0.9091\n",
      "Epoch 72/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.1800 - accuracy: 0.9384 - val_loss: 0.3257 - val_accuracy: 0.9091\n",
      "Epoch 73/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.1756 - accuracy: 0.9392 - val_loss: 0.3148 - val_accuracy: 0.9091\n",
      "Epoch 74/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.1652 - accuracy: 0.9427 - val_loss: 0.3213 - val_accuracy: 0.9141\n",
      "Epoch 75/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.1761 - accuracy: 0.9401 - val_loss: 0.3429 - val_accuracy: 0.9105\n",
      "Epoch 76/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.1585 - accuracy: 0.9460 - val_loss: 0.3145 - val_accuracy: 0.9198\n",
      "Epoch 77/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.1509 - accuracy: 0.9485 - val_loss: 0.3253 - val_accuracy: 0.9105\n",
      "Epoch 78/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.1461 - accuracy: 0.9503 - val_loss: 0.3090 - val_accuracy: 0.9198\n",
      "Epoch 79/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.1363 - accuracy: 0.9553 - val_loss: 0.3128 - val_accuracy: 0.9198\n",
      "Epoch 80/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.1308 - accuracy: 0.9558 - val_loss: 0.3005 - val_accuracy: 0.9313\n",
      "Epoch 81/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.1426 - accuracy: 0.9440 - val_loss: 0.3351 - val_accuracy: 0.9141\n",
      "Epoch 82/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.1404 - accuracy: 0.9495 - val_loss: 0.3542 - val_accuracy: 0.9034\n",
      "Epoch 83/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.1309 - accuracy: 0.9547 - val_loss: 0.3185 - val_accuracy: 0.9198\n",
      "Epoch 84/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.1341 - accuracy: 0.9563 - val_loss: 0.3199 - val_accuracy: 0.9184\n",
      "Epoch 85/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.1427 - accuracy: 0.9520 - val_loss: 0.3615 - val_accuracy: 0.9069\n",
      "Epoch 86/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.1331 - accuracy: 0.9515 - val_loss: 0.3223 - val_accuracy: 0.9198\n",
      "Epoch 87/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.1310 - accuracy: 0.9540 - val_loss: 0.3260 - val_accuracy: 0.9162\n",
      "Epoch 88/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.1163 - accuracy: 0.9617 - val_loss: 0.3354 - val_accuracy: 0.9155\n",
      "Epoch 89/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.1149 - accuracy: 0.9601 - val_loss: 0.3181 - val_accuracy: 0.9184\n",
      "Epoch 90/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.1175 - accuracy: 0.9613 - val_loss: 0.3464 - val_accuracy: 0.9170\n",
      "Epoch 91/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.1325 - accuracy: 0.9549 - val_loss: 0.3047 - val_accuracy: 0.9298\n",
      "Epoch 92/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.1107 - accuracy: 0.9621 - val_loss: 0.3484 - val_accuracy: 0.9084\n",
      "Epoch 93/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.1127 - accuracy: 0.9606 - val_loss: 0.3214 - val_accuracy: 0.9241\n",
      "Epoch 94/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.1093 - accuracy: 0.9639 - val_loss: 0.3185 - val_accuracy: 0.9270\n",
      "Epoch 95/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.1114 - accuracy: 0.9603 - val_loss: 0.3187 - val_accuracy: 0.9177\n",
      "Epoch 96/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.1051 - accuracy: 0.9649 - val_loss: 0.3084 - val_accuracy: 0.9205\n",
      "Epoch 97/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.1100 - accuracy: 0.9658 - val_loss: 0.2989 - val_accuracy: 0.9306\n",
      "Epoch 98/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.0988 - accuracy: 0.9640 - val_loss: 0.3497 - val_accuracy: 0.9112\n",
      "Epoch 99/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.1100 - accuracy: 0.9613 - val_loss: 0.3231 - val_accuracy: 0.9184\n",
      "Epoch 100/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.0951 - accuracy: 0.9647 - val_loss: 0.3375 - val_accuracy: 0.9162\n",
      "Epoch 101/150\n",
      "22/22 [==============================] - 1s 39ms/step - loss: 0.0915 - accuracy: 0.9683 - val_loss: 0.3434 - val_accuracy: 0.9205\n",
      "Epoch 102/150\n",
      "22/22 [==============================] - 1s 39ms/step - loss: 0.1131 - accuracy: 0.9606 - val_loss: 0.3028 - val_accuracy: 0.9198\n",
      "Epoch 103/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.1045 - accuracy: 0.9655 - val_loss: 0.3396 - val_accuracy: 0.9191\n",
      "Epoch 104/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.0930 - accuracy: 0.9681 - val_loss: 0.3065 - val_accuracy: 0.9327\n",
      "Epoch 105/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.0921 - accuracy: 0.9714 - val_loss: 0.3442 - val_accuracy: 0.9162\n",
      "Epoch 106/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.0978 - accuracy: 0.9667 - val_loss: 0.3646 - val_accuracy: 0.9184\n",
      "Epoch 107/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.1029 - accuracy: 0.9642 - val_loss: 0.3187 - val_accuracy: 0.9263\n",
      "Epoch 108/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.0915 - accuracy: 0.9683 - val_loss: 0.3238 - val_accuracy: 0.9198\n",
      "Epoch 109/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.1002 - accuracy: 0.9676 - val_loss: 0.2943 - val_accuracy: 0.9248\n",
      "Epoch 110/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.0976 - accuracy: 0.9647 - val_loss: 0.3214 - val_accuracy: 0.9248\n",
      "Epoch 111/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.0924 - accuracy: 0.9705 - val_loss: 0.3035 - val_accuracy: 0.9277\n",
      "Epoch 112/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.0791 - accuracy: 0.9742 - val_loss: 0.3114 - val_accuracy: 0.9270\n",
      "Epoch 113/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.0852 - accuracy: 0.9698 - val_loss: 0.3381 - val_accuracy: 0.9270\n",
      "Epoch 114/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.0816 - accuracy: 0.9723 - val_loss: 0.3659 - val_accuracy: 0.9184\n",
      "Epoch 115/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.0800 - accuracy: 0.9723 - val_loss: 0.3174 - val_accuracy: 0.9227\n",
      "Epoch 116/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.0822 - accuracy: 0.9710 - val_loss: 0.3162 - val_accuracy: 0.9263\n",
      "Epoch 117/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.0787 - accuracy: 0.9737 - val_loss: 0.3198 - val_accuracy: 0.9291\n",
      "Epoch 118/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.0799 - accuracy: 0.9707 - val_loss: 0.3254 - val_accuracy: 0.9277\n",
      "Epoch 119/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.0743 - accuracy: 0.9741 - val_loss: 0.3312 - val_accuracy: 0.9248\n",
      "Epoch 120/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.0833 - accuracy: 0.9701 - val_loss: 0.3339 - val_accuracy: 0.9248\n",
      "Epoch 121/150\n",
      "22/22 [==============================] - 1s 39ms/step - loss: 0.0913 - accuracy: 0.9685 - val_loss: 0.3283 - val_accuracy: 0.9327\n",
      "Epoch 122/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.0811 - accuracy: 0.9723 - val_loss: 0.3393 - val_accuracy: 0.9234\n",
      "Epoch 123/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.0777 - accuracy: 0.9719 - val_loss: 0.2953 - val_accuracy: 0.9291\n",
      "Epoch 124/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.0789 - accuracy: 0.9726 - val_loss: 0.2952 - val_accuracy: 0.9349\n",
      "Epoch 125/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.0738 - accuracy: 0.9755 - val_loss: 0.3110 - val_accuracy: 0.9298\n",
      "Epoch 126/150\n",
      "22/22 [==============================] - 1s 39ms/step - loss: 0.0685 - accuracy: 0.9775 - val_loss: 0.2955 - val_accuracy: 0.9306\n",
      "Epoch 127/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.0731 - accuracy: 0.9755 - val_loss: 0.3209 - val_accuracy: 0.9263\n",
      "Epoch 128/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.0677 - accuracy: 0.9776 - val_loss: 0.3481 - val_accuracy: 0.9263\n",
      "Epoch 129/150\n",
      "22/22 [==============================] - 1s 39ms/step - loss: 0.0736 - accuracy: 0.9742 - val_loss: 0.3111 - val_accuracy: 0.9313\n",
      "Epoch 130/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.0765 - accuracy: 0.9733 - val_loss: 0.3419 - val_accuracy: 0.9234\n",
      "Epoch 131/150\n",
      "22/22 [==============================] - 1s 39ms/step - loss: 0.0722 - accuracy: 0.9741 - val_loss: 0.3176 - val_accuracy: 0.9270\n",
      "Epoch 132/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.0679 - accuracy: 0.9782 - val_loss: 0.2874 - val_accuracy: 0.9363\n",
      "Epoch 133/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.0740 - accuracy: 0.9749 - val_loss: 0.3263 - val_accuracy: 0.9248\n",
      "Epoch 134/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.0748 - accuracy: 0.9766 - val_loss: 0.3036 - val_accuracy: 0.9306\n",
      "Epoch 135/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.0650 - accuracy: 0.9771 - val_loss: 0.3379 - val_accuracy: 0.9320\n",
      "Epoch 136/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.0678 - accuracy: 0.9771 - val_loss: 0.3070 - val_accuracy: 0.9284\n",
      "Epoch 137/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.0692 - accuracy: 0.9749 - val_loss: 0.3267 - val_accuracy: 0.9277\n",
      "Epoch 138/150\n",
      "22/22 [==============================] - 1s 39ms/step - loss: 0.0580 - accuracy: 0.9810 - val_loss: 0.3315 - val_accuracy: 0.9320\n",
      "Epoch 139/150\n",
      "22/22 [==============================] - 1s 39ms/step - loss: 0.0567 - accuracy: 0.9796 - val_loss: 0.3414 - val_accuracy: 0.9256\n",
      "Epoch 140/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.0755 - accuracy: 0.9742 - val_loss: 0.3227 - val_accuracy: 0.9356\n",
      "Epoch 141/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.0750 - accuracy: 0.9715 - val_loss: 0.3390 - val_accuracy: 0.9263\n",
      "Epoch 142/150\n",
      "22/22 [==============================] - 1s 39ms/step - loss: 0.0711 - accuracy: 0.9748 - val_loss: 0.3117 - val_accuracy: 0.9320\n",
      "Epoch 143/150\n",
      "22/22 [==============================] - 1s 39ms/step - loss: 0.0659 - accuracy: 0.9760 - val_loss: 0.3270 - val_accuracy: 0.9298\n",
      "Epoch 144/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.0550 - accuracy: 0.9803 - val_loss: 0.2960 - val_accuracy: 0.9320\n",
      "Epoch 145/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.0465 - accuracy: 0.9851 - val_loss: 0.3350 - val_accuracy: 0.9341\n",
      "Epoch 146/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.0567 - accuracy: 0.9796 - val_loss: 0.3349 - val_accuracy: 0.9263\n",
      "Epoch 147/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.0598 - accuracy: 0.9812 - val_loss: 0.3296 - val_accuracy: 0.9227\n",
      "Epoch 148/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.0515 - accuracy: 0.9821 - val_loss: 0.3441 - val_accuracy: 0.9277\n",
      "Epoch 149/150\n",
      "22/22 [==============================] - 1s 39ms/step - loss: 0.0610 - accuracy: 0.9821 - val_loss: 0.3131 - val_accuracy: 0.9284\n",
      "Epoch 150/150\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.0693 - accuracy: 0.9775 - val_loss: 0.3315 - val_accuracy: 0.9284\n",
      "Trained the model in: 0:02:06.925878\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model_tunned.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "checkpointer = ModelCheckpoint(\n",
    "    filepath='saved_models_CNN/weights.best.basic_cnn.hdf5',\n",
    "    verbose=1,\n",
    "    save_best_only=True\n",
    ")\n",
    "start = datetime.now()\n",
    "model_tunned.fit(x_train, y_train, batch_size=num_batch_size, epochs=150, validation_data=(x_val, y_val), verbose=1)\n",
    "duration = datetime.now() - start\n",
    "print(f'Trained the model in: {duration}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9973157048225403\n",
      "Validation accuracy: 0.9284180402755737\n"
     ]
    }
   ],
   "source": [
    "score_train = model_tunned.evaluate(x_train, y_train, verbose=0)\n",
    "print(f'Training accuracy: {score_train[1]}')\n",
    "score_val = model_tunned.evaluate(x_val, y_val, verbose=0)\n",
    "print(f'Validation accuracy: {score_val[1]}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Testing the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class is: air_conditioner \n",
      "\n",
      "air_conditioner \t\t :  0.99999630451202392578125000000000\n",
      "car_horn \t\t :  0.00000006697479904005376738496125\n",
      "children_playing \t\t :  0.00000000167404823336880781425862\n",
      "dog_bark \t\t :  0.00000000000075765787957773889083\n",
      "drilling \t\t :  0.00000021185903165132913272827864\n",
      "engine_idling \t\t :  0.00000014702264650168217485770583\n",
      "gun_shot \t\t :  0.00000000366393448913981956138741\n",
      "jackhammer \t\t :  0.00000017999784063249535392969847\n",
      "siren \t\t :  0.00000001099093882572788061224855\n",
      "street_music \t\t :  0.00000296965458801423665136098862\n"
     ]
    }
   ],
   "source": [
    "# Air conditioner\n",
    "file_name = 'samples/100852-0-0-0.wav'\n",
    "print_prediction(file_name, model_tunned)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class is: drilling \n",
      "\n",
      "air_conditioner \t\t :  0.00000000000000026979293742271989\n",
      "car_horn \t\t :  0.00000000000024457202213966355853\n",
      "children_playing \t\t :  0.00000000000000706878386526983357\n",
      "dog_bark \t\t :  0.00000000000000000018919165110388\n",
      "drilling \t\t :  1.00000000000000000000000000000000\n",
      "engine_idling \t\t :  0.00000000000000000346369101566116\n",
      "gun_shot \t\t :  0.00000000000000021787157799637479\n",
      "jackhammer \t\t :  0.00000000378456110894376251962967\n",
      "siren \t\t :  0.00000000000000000009114571984047\n",
      "street_music \t\t :  0.00000000059555377296405254128331\n"
     ]
    }
   ],
   "source": [
    "# Drilling\n",
    "file_name = 'samples/103199-4-0-0.wav'\n",
    "print_prediction(file_name, model_tunned)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class is: street_music \n",
      "\n",
      "air_conditioner \t\t :  0.00000001049017406273833330487832\n",
      "car_horn \t\t :  0.00000000674281652734975978091825\n",
      "children_playing \t\t :  0.00010638760431902483105659484863\n",
      "dog_bark \t\t :  0.00000350892128153645899146795273\n",
      "drilling \t\t :  0.00000001670273874765371147077531\n",
      "engine_idling \t\t :  0.00000000044320253023144573489844\n",
      "gun_shot \t\t :  0.00000000000000000014077409443886\n",
      "jackhammer \t\t :  0.00000000001790111982258313361172\n",
      "siren \t\t :  0.00000002098840390374334674561396\n",
      "street_music \t\t :  0.99989008903503417968750000000000\n"
     ]
    }
   ],
   "source": [
    "# Street music\n",
    "file_name = 'samples/101848-9-0-0.wav'\n",
    "print_prediction(file_name, model_tunned)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class is: car_horn \n",
      "\n",
      "air_conditioner \t\t :  0.00008723148494027554988861083984\n",
      "car_horn \t\t :  0.77237874269485473632812500000000\n",
      "children_playing \t\t :  0.00090009614359587430953979492188\n",
      "dog_bark \t\t :  0.09703276306390762329101562500000\n",
      "drilling \t\t :  0.07019501924514770507812500000000\n",
      "engine_idling \t\t :  0.00024893652880564332008361816406\n",
      "gun_shot \t\t :  0.01377445738762617111206054687500\n",
      "jackhammer \t\t :  0.04301467537879943847656250000000\n",
      "siren \t\t :  0.00208531599491834640502929687500\n",
      "street_music \t\t :  0.00028278314857743680477142333984\n"
     ]
    }
   ],
   "source": [
    "# Car horn\n",
    "file_name = 'samples/100648-1-0-0.wav'\n",
    "print_prediction(file_name, model_tunned)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Matriz de confusion"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      " air_conditioner       0.94      0.95      0.94       203\n",
      "        car_horn       0.92      0.94      0.93        86\n",
      "children_playing       0.85      0.91      0.88       183\n",
      "        dog_bark       0.97      0.86      0.91       201\n",
      "        drilling       0.86      0.95      0.90       206\n",
      "   engine_idling       0.98      0.95      0.97       193\n",
      "        gun_shot       0.96      0.96      0.96        72\n",
      "      jackhammer       0.98      0.90      0.94       208\n",
      "           siren       0.94      0.98      0.96       165\n",
      "    street_music       0.87      0.87      0.87       230\n",
      "\n",
      "        accuracy                           0.92      1747\n",
      "       macro avg       0.93      0.93      0.93      1747\n",
      "    weighted avg       0.92      0.92      0.92      1747\n",
      "\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix(x_test, y_test, model_tunned)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Heatmap"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "heat_map(x_test, y_test, model_tunned)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Saving the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "model_json = model_tunned.to_json()\n",
    "with open('models/cnn_tunned.json', 'w') as json_file:\n",
    "    json_file.write(model_json)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "# Serialize weights to HDF5\n",
    "model_tunned.save_weights('models/cnn_tunned.h5')\n",
    "print('Model saved')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}